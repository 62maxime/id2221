{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop Logo](/files/logo/hadoop.png)\n",
    "# **Lab 1 - Hadoop MapReduce and HDFS**\n",
    "#### The following steps (Part 1 and Part 2) demonstrate how to install HDFS and create and run \"word count\" application with Hadoop MapReduce. Then, in Part 3, you are asked to implement ... with Hadoop MapReduce.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: HDFS **\n",
    "\n",
    "#### Install HDFS\n",
    "\n",
    "1. Download the Hadoop platform from the following link:\n",
    "[Hadoop](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz)\n",
    "\n",
    "2. Set the environment variable.\n",
    "   ```bash\n",
    "   export JAVA_HOME=\"<JAVA PATH>\"\n",
    "   export HADOOP_HOME=\"<HADOOP PATH>/hadoop-2.6.4\"\n",
    "   export HADOOP_CONFIG=\"$HADOOP_HOME/etc/hadoop\"\n",
    "   ```\n",
    "\n",
    "3. Specify environment variables in `$HADOOP_CONFIG/hadoop-env.sh`.\n",
    "   ```bash\n",
    "   export JAVA_HOME=\"<JAVA PATH>\"\n",
    "   ```\n",
    "\n",
    "4. Make three folders on local file system, where HDFS namenode and datanode store their data.\n",
    "   ```bash\n",
    "   mkdir -p $HADOOP_HOME/hdfs/namenode\n",
    "  \n",
    "   mkdir -p $HADOOP_HOME/hdfs/datanode\n",
    "   ```\n",
    "\n",
    "5. The main HDFS configuration file is located at `$HADOOP_CONFIG/hdfs-site.xml`. Specify the folders path, built in step 4.\n",
    "   ```xml\n",
    "   <configuration>\n",
    "   <property>\n",
    "      <name>dfs.namenode.name.dir</name>\n",
    "      <value>file:///<HADOOP HOME PATH>/hdfs/namenode</value>\n",
    "      <description>Path on the local filesystem where the NameNode stores the namespace and transaction logs persistently.</description>\n",
    "   </property>\n",
    "\n",
    "   <property>\n",
    "      <name>dfs.datanode.data.dir</name>\n",
    "      <value>file:///<HADOOP HOME PATH>/hdfs/datanode</value>\n",
    "      <description>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.</description>\n",
    "   </property>\n",
    "   </configuration>\n",
    "   ```\n",
    "\n",
    "6. Specify the address of the namenode (master) in `$HADOOP_CONFIG/core-site.sh`.\n",
    "   ```xml\n",
    "   <configuration>\n",
    "   <property>\n",
    "      <name>fs.defaultFS</name>\n",
    "      <value>hdfs://127.0.0.1:9000</value>\n",
    "      <description>NameNode URI</description>\n",
    "   </property>\n",
    "   </configuration>\n",
    "   ```\n",
    "\n",
    "7. Format the namenode directory (DO THIS ONLY ONCE, THE FIRST TIME).\n",
    "   ```bash\n",
    "   $HADOOP_HOME/bin/hdfs namenode -format\n",
    "   ```\n",
    "\n",
    "8. Start the namenode and datanode daemons\n",
    "   ```bash\n",
    "   $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode\n",
    "\n",
    "   $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode\n",
    "   ```\n",
    "\n",
    "#### Test HDFS\n",
    "1. Prints out the HDFS running processes, by running the `jps` command in a terminal.\n",
    "2. Monitor the process through their web interfaces:\n",
    "   - Namenode: [http://127.0.0.1:50070](http://127.0.0.1:50070)\n",
    "   - Datanode: [http://127.0.0.1:50075](http://127.0.0.1:50075)\n",
    "3. Try HDFS commands\n",
    "   ```bash\n",
    "   # Create a new directory /sics on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -mkdir /sics\n",
    "\n",
    "   # Create a file, call it big, on your local filesystem and upload it to HDFS under /sics.\n",
    "   $HADOOP_HOME/bin/hdfs dfs -put big /sics\n",
    "\n",
    "   # View the content of /sics directory\n",
    "   $HADOOP_HOME/bin/hdfs dfs -ls big /sics\n",
    "\n",
    "   # Determine the size of big on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -du -h /sics/big\n",
    "\n",
    "   # Print the first 5 lines to screen from big on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -cat /sics/big | head -n 5\n",
    "   \n",
    "   # Copy big to /big hdfscopy on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -cp /sics/big /sics/big_hdfscopy\n",
    "   \n",
    "   # Copy big back to local filesystem and name it big localcopy\n",
    "   $HADOOP_HOME/bin/hdfs dfs -get /sics/big big_localcopy\n",
    "   \n",
    "   # Check the entire HDFS filesystem for inconsistencies/problems\n",
    "   $HADOOP_HOME/bin/hdfs fsck /\n",
    "   \n",
    "   # Delete big from HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -rm /sics/big\n",
    "   \n",
    "   # Delete /sics directory from HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -rm -r /sics\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing example with Flink, Kafka and Python\n",
    "\n",
    "This repository contains the components for a simple streaming pipeline:\n",
    " * Generate data and write it to Apache Kafka\n",
    " * Process the generated data from Kafka using Apache Flink\n",
    " * Write the results back to Kafka for further processing\n",
    " * Analyze the results from Kafka using Ipython Notebook\n",
    "\n",
    "**Questions and feedback can be sent to [gyfora@apache.org](mailto:gyfora@apache.org)**\n",
    "\n",
    "### Description\n",
    "\n",
    "In this very simple example we will analyse temperature data, generated for European cities in a streaming fashion.\n",
    "\n",
    "Data is generated as simple Strings in the format of: `\"City, Temperature\"`\n",
    "<br>`\"Budapest, 30\", \"Stockholm, 20\", \"Budapest, 32\" …` \n",
    "\n",
    "Our goal is to analyse the incoming data in a continuous fashion, updating our statistics as new data becomes available. \n",
    "\n",
    "We generate random temperatures using a [simple Scala program](https://github.com/gyfora/summer-school/blob/master/flink/src/main/scala/summerschool/DataGenerator.scala), which writes it directly to Kafka, making it available for processing.\n",
    "\n",
    "We then use a [Flink program](https://github.com/gyfora/summer-school/blob/master/flink/src/main/scala/summerschool/FlinkKafkaExample.scala) to do the following processing steps:\n",
    "\n",
    "1. Parse the incoming String into the Scala case class `Temp(city: String, temp: Double)`:\n",
    "\n",
    "   ```\n",
    "   \"Budapest, 30\" -> (\"Budapest\", 30)\n",
    "   \"Stockholm, 20\" -> (\"Stockholm\", 20)\n",
    "   ```\n",
    "   \n",
    "   *Note: An important consideration we need to make when implementing the parsing step is that it should be robust to errors coming from incorrect input formats.*\n",
    "2. We compute a historical average of the temperatures for each city:\n",
    "\n",
    "   ```\n",
    "   (\"Budapest\", 30) -> Avg: (“Budapest\", 30)\n",
    "   (\"Budapest\", 40) -> Avg: (“Budapest\", 35)\n",
    "   (“Stockholm”, 20) -> Avg: (“Stockholm”, 20)\n",
    "   (\"Budapest\", 37) -> Avg: (“Budapest\", 35.67)\n",
    "   (“Stockholm”, 22) -> Avg: (“Stockholm”, 21)\n",
    "   ```\n",
    "   *Note: There is distinctive property of computing a historical average compared to the parsing operator. Here we need to keep some computational state for each city (sum and count) so we are able to update the average when the next element arrives.*\n",
    "\n",
    "    *You can read more about the interesting topic of operator states and stateful stream processing in [this wiki article](https://cwiki.apache.org/confluence/display/FLINK/Stateful+Stream+Processing).*\n",
    "3. We compute the current global maximum temperature every 5 seconds:\n",
    "   \n",
    "    ```\n",
    "   (\"Budapest\", 32) \n",
    "   (\"Madrid\", 34)     -> Max: (“Madrid\", 34)\n",
    "   (\"Stockholm\", 20)\n",
    "   -----------------------------------------\n",
    "   (\"Budapest\", 36) \n",
    "   (\"Madrid\", 33)     -> Max: (“Budapest\", 36)\n",
    "   (\"Stockholm\", 23)\n",
    "   ```\n",
    "   *Note: This is a typical example of window computations, when the data stream is discretised into small batches (windows) and some sort of aggregation or transformation is applied independently on each window.*\n",
    "4. The computed statistics are written back to Kafka and can be further analysed. In this example we use [IPython notebook](https://github.com/gyfora/summer-school/blob/master/python/KafkaExample.ipynb) to provide basic visualisation.\n",
    "\n",
    "### Running the example:\n",
    "\n",
    "**Prerequisites**: *JDK 7+, Maven 3.x, Scala 2.10, Python 2.7, IPython notebook*\n",
    "\n",
    " 1. Download & install [Apache Kafka](https://kafka.apache.org/08/quickstart.html)\n",
    " 2. Install the Python kafka client\n",
    "\n",
    "    ```bash\n",
    "    pip install kafka-python\n",
    "    ```\n",
    " 3. Clone the repository and build a jar\n",
    "\n",
    "     ```bash\n",
    "    git clone https://github.com/gyfora/summer-school.git\n",
    "    cd summer-school/flink\n",
    "    mvn assembly:assembly\n",
    "    ```\n",
    " 4. Start the Kafka server and create the topics\n",
    "\n",
    "     ```bash\n",
    "    cd <KAFKA-DIR>\n",
    "    # Start Zookeper server\n",
    "    bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "    # Start Kafka server\n",
    "    bin/kafka-server-start.sh config/server.properties\n",
    "    # Create input and output topics\n",
    "    bin/kafka-topics.sh --create --topic input --partitions 1 --replication-factor 1 --zookeeper localhost:2181\n",
    "    bin/kafka-topics.sh --create --topic output_avg --partitions 1 --replication-factor 1 --zookeeper localhost:2181\n",
    "    bin/kafka-topics.sh --create --topic output_max --partitions 1 --replication-factor 1 --zookeeper localhost:2181\n",
    "    ```\n",
    " 5. Start Flink streaming job\n",
    "\n",
    "    ```bash\n",
    "    # Running on a Flink mini cluster (equivalent to executing from the IDE) \n",
    "    cd <git>/summer-school/flink\n",
    "    java -classpath target/FlinkExample.jar summerschool.FlinkKafkaExample\n",
    "    ```\n",
    "\n",
    "    To execute the job on an already running cluster ([cluster setup guide](https://ci.apache.org/projects/flink/flink-docs-master/setup/local_setup.html)) we can either use the Flink command-line or web-client:\n",
    "\n",
    "    ```bash\n",
    "    # Start a local Flink cluster\n",
    "    cd <FLINK-DIR>\n",
    "    bin/start-cluster-streaming.sh\n",
    "\n",
    "    # Run the example job using the command-line client\n",
    "    bin/flink run -c summerschool.FlinkKafkaExample <summer-school-dir>/flink/target/FlinkExample.jar\n",
    "    ```\n",
    "\n",
    " 6. Start Data generator\n",
    "\n",
    "    ```bash\n",
    "    cd <git>/summer-school/flink\n",
    "    java -classpath target/FlinkExample.jar summerschool.DataGenerator\n",
    "    ```\n",
    " 7. Start IPython Notebook to further analyse the results\n",
    "\n",
    "    ```bash\n",
    "    cd <git>/summer-school/python\n",
    "    ipython notebook\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
