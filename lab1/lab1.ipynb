{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop Logo](/files/logo/hadoop.png)\n",
    "# **Lab 1 - Hadoop MapReduce and HDFS**\n",
    "#### The following steps (Part 1 and Part 2) demonstrate how to install HDFS and create and run \"word count\" application with Hadoop MapReduce. Then, in Part 3, you are asked to implement ... with Hadoop MapReduce.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: HDFS **\n",
    "\n",
    "#### Install HDFS\n",
    "\n",
    "1. Download the Hadoop platform from the following link:\n",
    "[Hadoop](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz)\n",
    "\n",
    "2. Set the environment variable.\n",
    "   ```bash\n",
    "   export JAVA_HOME=\"<JAVA PATH>\"\n",
    "   export HADOOP_HOME=\"<HADOOP PATH>/hadoop-2.6.4\"\n",
    "   export HADOOP_CONFIG=\"$HADOOP_HOME/etc/hadoop\"\n",
    "   ```\n",
    "\n",
    "3. Specify environment variables in `$HADOOP_CONFIG/hadoop-env.sh`.\n",
    "   ```bash\n",
    "   export JAVA_HOME=\"<JAVA PATH>\"\n",
    "   ```\n",
    "\n",
    "4. Make three folders on local file system, where HDFS namenode and datanode store their data.\n",
    "   ```bash\n",
    "   mkdir -p $HADOOP_HOME/hdfs/namenode\n",
    "  \n",
    "   mkdir -p $HADOOP_HOME/hdfs/datanode\n",
    "   ```\n",
    "\n",
    "5. The main HDFS configuration file is located at `$HADOOP_CONFIG/hdfs-site.xml`. Specify the folders path, built in step 4.\n",
    "   ```xml\n",
    "   <configuration>\n",
    "   <property>\n",
    "      <name>dfs.namenode.name.dir</name>\n",
    "      <value>file:///<HADOOP HOME PATH>/hdfs/namenode</value>\n",
    "      <description>Path on the local filesystem where the NameNode stores the namespace and transaction logs persistently.</description>\n",
    "   </property>\n",
    "\n",
    "   <property>\n",
    "      <name>dfs.datanode.data.dir</name>\n",
    "      <value>file:///<HADOOP HOME PATH>/hdfs/datanode</value>\n",
    "      <description>Comma separated list of paths on the local filesystem of a DataNode where it should store its blocks.</description>\n",
    "   </property>\n",
    "   </configuration>\n",
    "   ```\n",
    "\n",
    "6. Specify the address of the namenode (master) in `$HADOOP_CONFIG/core-site.sh`.\n",
    "   ```xml\n",
    "   <configuration>\n",
    "   <property>\n",
    "      <name>fs.defaultFS</name>\n",
    "      <value>hdfs://127.0.0.1:9000</value>\n",
    "      <description>NameNode URI</description>\n",
    "   </property>\n",
    "   </configuration>\n",
    "   ```\n",
    "\n",
    "7. Format the namenode directory (DO THIS ONLY ONCE, THE FIRST TIME).\n",
    "   ```bash\n",
    "   $HADOOP_HOME/bin/hdfs namenode -format\n",
    "   ```\n",
    "\n",
    "8. Start the namenode and datanode daemons\n",
    "   ```bash\n",
    "   $HADOOP_HOME/sbin/hadoop-daemon.sh start namenode\n",
    "\n",
    "   $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode\n",
    "   ```\n",
    "\n",
    "#### Test HDFS\n",
    "1. Prints out the HDFS running processes, by running the `jps` command in a terminal.\n",
    "2. Monitor the process through their web interfaces:\n",
    "   - Namenode: [http://127.0.0.1:50070](http://127.0.0.1:50070)\n",
    "   - Datanode: [http://127.0.0.1:50075](http://127.0.0.1:50075)\n",
    "3. Try HDFS commands\n",
    "   ```bash\n",
    "   # Create a new directory /sics on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -mkdir /sics\n",
    "\n",
    "   # Create a file, call it big, on your local filesystem and upload it to HDFS under /sics.\n",
    "   $HADOOP_HOME/bin/hdfs dfs -put big /sics\n",
    "\n",
    "   # View the content of /sics directory\n",
    "   $HADOOP_HOME/bin/hdfs dfs -ls big /sics\n",
    "\n",
    "   # Determine the size of big on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -du -h /sics/big\n",
    "\n",
    "   # Print the first 5 lines to screen from big on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -cat /sics/big | head -n 5\n",
    "   \n",
    "   # Copy big to /big hdfscopy on HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -cp /sics/big /sics/big_hdfscopy\n",
    "   \n",
    "   # Copy big back to local filesystem and name it big localcopy\n",
    "   $HADOOP_HOME/bin/hdfs dfs -get /sics/big big_localcopy\n",
    "   \n",
    "   # Check the entire HDFS filesystem for inconsistencies/problems\n",
    "   $HADOOP_HOME/bin/hdfs fsck /\n",
    "   \n",
    "   # Delete big from HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -rm /sics/big\n",
    "   \n",
    "   # Delete /sics directory from HDFS\n",
    "   $HADOOP_HOME/bin/hdfs dfs -rm -r /sics\n",
    "   ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
