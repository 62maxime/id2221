{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing example with Flink, Kafka and Python\n",
    "\n",
    "This repository contains the components for a simple streaming pipeline:\n",
    " * Generate data and write it to Apache Kafka\n",
    " * Process the generated data from Kafka using Apache Flink\n",
    " * Write the results back to Kafka for further processing\n",
    " * Analyze the results from Kafka using Ipython Notebook\n",
    "\n",
    "**Questions and feedback can be sent to [gyfora@apache.org](mailto:gyfora@apache.org)**\n",
    "\n",
    "### Description\n",
    "\n",
    "In this very simple example we will analyse temperature data, generated for European cities in a streaming fashion.\n",
    "\n",
    "Data is generated as simple Strings in the format of: `\"City, Temperature\"`\n",
    "<br>`\"Budapest, 30\", \"Stockholm, 20\", \"Budapest, 32\" …` \n",
    "\n",
    "Our goal is to analyse the incoming data in a continuous fashion, updating our statistics as new data becomes available. \n",
    "\n",
    "We generate random temperatures using a [simple Scala program](https://github.com/gyfora/summer-school/blob/master/flink/src/main/scala/summerschool/DataGenerator.scala), which writes it directly to Kafka, making it available for processing.\n",
    "\n",
    "We then use a [Flink program](https://github.com/gyfora/summer-school/blob/master/flink/src/main/scala/summerschool/FlinkKafkaExample.scala) to do the following processing steps:\n",
    "\n",
    "1. Parse the incoming String into the Scala case class `Temp(city: String, temp: Double)`:\n",
    "\n",
    "   ```\n",
    "   \"Budapest, 30\" -> (\"Budapest\", 30)\n",
    "   \"Stockholm, 20\" -> (\"Stockholm\", 20)\n",
    "   ```\n",
    "   \n",
    "   *Note: An important consideration we need to make when implementing the parsing step is that it should be robust to errors coming from incorrect input formats.*\n",
    "2. We compute a historical average of the temperatures for each city:\n",
    "\n",
    "   ```\n",
    "   (\"Budapest\", 30) -> Avg: (“Budapest\", 30)\n",
    "   (\"Budapest\", 40) -> Avg: (“Budapest\", 35)\n",
    "   (“Stockholm”, 20) -> Avg: (“Stockholm”, 20)\n",
    "   (\"Budapest\", 37) -> Avg: (“Budapest\", 35.67)\n",
    "   (“Stockholm”, 22) -> Avg: (“Stockholm”, 21)\n",
    "   ```\n",
    "   *Note: There is distinctive property of computing a historical average compared to the parsing operator. Here we need to keep some computational state for each city (sum and count) so we are able to update the average when the next element arrives.*\n",
    "\n",
    "    *You can read more about the interesting topic of operator states and stateful stream processing in [this wiki article](https://cwiki.apache.org/confluence/display/FLINK/Stateful+Stream+Processing).*\n",
    "3. We compute the current global maximum temperature every 5 seconds:\n",
    "   \n",
    "    ```\n",
    "   (\"Budapest\", 32) \n",
    "   (\"Madrid\", 34)     -> Max: (“Madrid\", 34)\n",
    "   (\"Stockholm\", 20)\n",
    "   -----------------------------------------\n",
    "   (\"Budapest\", 36) \n",
    "   (\"Madrid\", 33)     -> Max: (“Budapest\", 36)\n",
    "   (\"Stockholm\", 23)\n",
    "   ```\n",
    "   *Note: This is a typical example of window computations, when the data stream is discretised into small batches (windows) and some sort of aggregation or transformation is applied independently on each window.*\n",
    "4. The computed statistics are written back to Kafka and can be further analysed. In this example we use [IPython notebook](https://github.com/gyfora/summer-school/blob/master/python/KafkaExample.ipynb) to provide basic visualisation.\n",
    "\n",
    "### Running the example:\n",
    "\n",
    "**Prerequisites**: *JDK 7+, Maven 3.x, Scala 2.10, Python 2.7, IPython notebook*\n",
    "\n",
    " 1. Download & install [Apache Kafka](https://kafka.apache.org/08/quickstart.html)\n",
    " 2. Install the Python kafka client\n",
    "\n",
    "    ```bash\n",
    "    pip install kafka-python\n",
    "    ```\n",
    " 3. Clone the repository and build a jar\n",
    "\n",
    "     ```bash\n",
    "    git clone https://github.com/gyfora/summer-school.git\n",
    "    cd summer-school/flink\n",
    "    mvn assembly:assembly\n",
    "    ```\n",
    " 4. Start the Kafka server and create the topics\n",
    "\n",
    "     ```bash\n",
    "    cd <KAFKA-DIR>\n",
    "    # Start Zookeper server\n",
    "    bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "    # Start Kafka server\n",
    "    bin/kafka-server-start.sh config/server.properties\n",
    "    # Create input and output topics\n",
    "    bin/kafka-topics.sh --create --topic input --partitions 1 --replication-factor 1 --zookeeper localhost:2181\n",
    "    bin/kafka-topics.sh --create --topic output_avg --partitions 1 --replication-factor 1 --zookeeper localhost:2181\n",
    "    bin/kafka-topics.sh --create --topic output_max --partitions 1 --replication-factor 1 --zookeeper localhost:2181\n",
    "    ```\n",
    " 5. Start Flink streaming job\n",
    "\n",
    "    ```bash\n",
    "    # Running on a Flink mini cluster (equivalent to executing from the IDE) \n",
    "    cd <git>/summer-school/flink\n",
    "    java -classpath target/FlinkExample.jar summerschool.FlinkKafkaExample\n",
    "    ```\n",
    "\n",
    "    To execute the job on an already running cluster ([cluster setup guide](https://ci.apache.org/projects/flink/flink-docs-master/setup/local_setup.html)) we can either use the Flink command-line or web-client:\n",
    "\n",
    "    ```bash\n",
    "    # Start a local Flink cluster\n",
    "    cd <FLINK-DIR>\n",
    "    bin/start-cluster-streaming.sh\n",
    "\n",
    "    # Run the example job using the command-line client\n",
    "    bin/flink run -c summerschool.FlinkKafkaExample <summer-school-dir>/flink/target/FlinkExample.jar\n",
    "    ```\n",
    "\n",
    " 6. Start Data generator\n",
    "\n",
    "    ```bash\n",
    "    cd <git>/summer-school/flink\n",
    "    java -classpath target/FlinkExample.jar summerschool.DataGenerator\n",
    "    ```\n",
    " 7. Start IPython Notebook to further analyse the results\n",
    "\n",
    "    ```bash\n",
    "    cd <git>/summer-school/python\n",
    "    ipython notebook\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
